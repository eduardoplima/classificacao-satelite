{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNnK685w5o2IoxcC8zLRp/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardoplima/classificacao-satelite/blob/main/classificacao-satelite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de Imagens de Satélite\n",
        "\n",
        "A classificação de imagens de satélite é uma tarefa crucial em diversas áreas, como monitoramento ambiental, planejamento urbano, agricultura de precisão e análise de desastres naturais. Essas imagens contêm informações valiosas sobre o uso da terra, cobertura vegetal e infraestrutura, que podem ser extraídas para criar mapas detalhados, identificar padrões e tomar decisões informadas. Tradicionalmente, a classificação de imagens de satélite exigia técnicas de processamento de imagem baseadas em regras ou algoritmos estatísticos. No entanto, com os avanços da Visão Computacional e a popularização do Deep Learning, novas abordagens baseadas em redes neurais convolucionais (CNNs) têm demonstrado um desempenho impressionante em termos de precisão e capacidade de generalização. As CNNs são especialmente eficazes para tarefas como a classificação de imagens de satélite, pois são capazes de aprender características relevantes das imagens sem a necessidade de engenharia manual de características.\n",
        "\n",
        "Uma abordagem recente que tem ganhado destaque para melhorar a performance de redes neurais em tarefas de classificação de imagens de satélite é o Transfer Learning. Essa técnica permite utilizar modelos previamente treinados em grandes conjuntos de dados (como o ImageNet) e ajustá-los para tarefas específicas, como a classificação de imagens de satélite. Modelos como o MobileNetV2, EfficientNetB0 e DenseNet121 são exemplos de arquiteturas que se destacam nesse contexto. O MobileNetV2 é uma rede neural eficiente e leve, ideal para dispositivos móveis e aplicações que requerem baixo custo computacional. O EfficientNetB0, por sua vez, é otimizado para obter a melhor relação entre acurácia e eficiência, oferecendo alto desempenho com um número reduzido de parâmetros. Já o DenseNet121 se caracteriza por um design de rede densa, onde cada camada é conectada a todas as camadas anteriores, facilitando a reutilização de características e permitindo melhores resultados em tarefas complexas. Essas arquiteturas, quando combinadas com o Transfer Learning, são extremamente poderosas para a classificação de imagens de satélite, permitindo resultados precisos mesmo com limitações de dados e recursos computacionais."
      ],
      "metadata": {
        "id": "KI6XrY99MZHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração\n",
        "\n",
        "Nessa seção configuramos nosso ambiente e importamos os pacotes necessários para o código a seguir"
      ],
      "metadata": {
        "id": "L7dcLO_Dw9RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow==2.13.0"
      ],
      "metadata": {
        "id": "srphNuWEr6e5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q lightgbm"
      ],
      "metadata": {
        "id": "xDhkCKcsDHJv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q kaggle\n",
        "#!pip install -q timm torch torchvision matplotlib scikit-learn\n"
      ],
      "metadata": {
        "id": "H3foDBr9xOrj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas padrão\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "# Machine Learning e Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import torch\n",
        "import timm\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "# Análise de dados e visualização\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "# Utilidades\n",
        "from PIL import Image\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "C53S7IuWw73j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# O dataset UC Merced Land Use Dataset\n",
        "\n",
        "O dataset utilizado nesse trabalho é o UC Merced Land Use Dataset. Trata-se de uma coleção de imagens de alta resolução capturadas por sensores remotos, projetada para pesquisas em classificação de uso da terra. Composta por 21 categorias distintas — como agrícola, aeroporto, campo de beisebol, praia, edifícios, floresta, rodovia, entre outras —, cada classe contém 100 imagens de 256×256 pixels. Essas imagens foram extraídas manualmente de grandes imagens da coleção Urban Area Imagery do USGS National Map, com resolução espacial de 1 pé (aproximadamente 30 cm por pixel), cobrindo diversas áreas urbanas dos Estados Unidos.\n",
        "\n",
        "Nas células a seguir fazemos o download do dataset a partir de um repositório Kaggle e convertemos as imagens para um formato legível pelo Tensorflow. Também exibimos algumas imagens aleatórias de teste."
      ],
      "metadata": {
        "id": "rhFCWgH88_Pd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxGj34UbZSqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "c82e0d88-8f4a-4d4e-9d91-3ad556ac2bf6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-23d8d09f-8687-4cb9-9d2c-781397827b84\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-23d8d09f-8687-4cb9-9d2c-781397827b84\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TCLdf7BrZrti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d abdulhasibuddin/uc-merced-land-use-dataset"
      ],
      "metadata": {
        "id": "tmEFEMsuShkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q uc-merced-land-use-dataset.zip -d uc-merced-land-use-dataset"
      ],
      "metadata": {
        "id": "k8RTs2CRlDF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in os.listdir('uc-merced-land-use-dataset/UCMerced_LandUse/Images'):\n",
        "    print(folder, \"→\", len(os.listdir(f\"uc-merced-land-use-dataset/UCMerced_LandUse/Images/{folder}\")), \"imagens\")"
      ],
      "metadata": {
        "id": "QjoAARopVTen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c1l86reCesY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "source_dir = \"uc-merced-land-use-dataset/UCMerced_LandUse/Images\"\n",
        "destination_dir = \"uc-merced-land-use-dataset/UCMerced_LandUse/ImagesJpg\"\n",
        "\n",
        "for folder in os.listdir(source_dir):\n",
        "    folder_path = os.path.join(source_dir, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".tif\"):\n",
        "                source_path = os.path.join(folder_path, filename)\n",
        "                name, ext = os.path.splitext(filename)\n",
        "                destination_path = os.path.join(folder_path, name + \".jpg\")\n",
        "\n",
        "                try:\n",
        "                    img = Image.open(source_path)\n",
        "                    img.save(destination_path, \"JPEG\")\n",
        "                except IOError as e:\n",
        "                    print(f\"Erro convertendo {source_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "dh0SttXDlg4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in os.listdir('uc-merced-land-use-dataset/UCMerced_LandUse/Images'):\n",
        "  path = f\"uc-merced-land-use-dataset/UCMerced_LandUse/Images/{folder}\"\n",
        "  image = Image.open(os.path.join(path, random.choice(os.listdir(path))))\n",
        "  plt.imshow(image)\n",
        "  plt.title(folder)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "a5-IyNC8VTSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criação de datasets de treinamento, validação e teste"
      ],
      "metadata": {
        "id": "oE0OGkVqyRWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'uc-merced-land-use-dataset/UCMerced_LandUse/Images'\n",
        "classes_name = [folder for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join('uc-merced-land-use-dataset/UCMerced_LandUse/Images', folder))]\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "seed = 123\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.3,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "test_ds = val_ds.take(val_batches // 2)\n",
        "val_ds = val_ds.skip(val_batches // 2)\n",
        "\n",
        "print(f\"Training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"Validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(f\"Test batches: {tf.data.experimental.cardinality(test_ds).numpy()}\")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "num_classes = 21\n",
        "\n",
        "\n",
        "translated_classes_names = [\n",
        " 'agrícola',\n",
        " 'avião',\n",
        " 'campo de beisebol',\n",
        " 'praia',\n",
        " 'edifícios',\n",
        " 'vegetação arbustiva',\n",
        " 'área residencial densa',\n",
        " 'floresta',\n",
        " 'autoestrada',\n",
        " 'campo de golfe',\n",
        " 'porto',\n",
        " 'cruzamento',\n",
        " 'área residencial média',\n",
        " 'parque de casas móveis',\n",
        " 'viaduto',\n",
        " 'estacionamento',\n",
        " 'rio',\n",
        " 'pista de pouso/decolagem',\n",
        " 'área residencial esparsa',\n",
        " 'tanques de armazenamento',\n",
        " 'quadra de tênis'\n",
        "]\n"
      ],
      "metadata": {
        "id": "wex9JWOewZzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação de uma rede neural convolucional simples"
      ],
      "metadata": {
        "id": "pZwTwSwWyfk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model = keras.Sequential([\n",
        "    layers.Rescaling(1./255, input_shape=(img_height,\n",
        "                                          img_width,\n",
        "                                          3)),\n",
        "\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Conv2D(128, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "simple_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.\\\n",
        "    SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "epochs = 50\n",
        "history = simple_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# 8. Evaluate on the Test set\n",
        "test_loss, test_acc = simple_model.evaluate(test_ds)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# 9. Plot training results (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tVeZzyGaf1at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Get predictions from the model\n",
        "for images, labels in test_ds:\n",
        "    preds = simple_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Create or append to a DataFrame\n",
        "results_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
        "results_df.loc[len(results_df)] = [\"CNN Simples\", accuracy, precision, recall, f1]"
      ],
      "metadata": {
        "id": "FBrpiFpxPo9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "_TgHIZd4lO6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = os.listdir('uc-merced-land-use-dataset/UCMerced_LandUse/Images')\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "cmd.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - CNN Simples\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Classe Predita\")\n",
        "plt.ylabel(\"Classe Verdadeira\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IIOL9_jJ26wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "RRp8AS4Su5oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de Transfer Learning para implementação de Rede Neural MobileNetV2"
      ],
      "metadata": {
        "id": "gRMWPqoAziZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Data augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip('horizontal_and_vertical'),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "# 3. Load the base model\n",
        "base_mobilenet_model = keras.applications.MobileNetV2(\n",
        "    input_shape=(256, 256, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_mobilenet_model.trainable = False  # Freeze the base model initially\n",
        "\n",
        "# 4. Create the full model\n",
        "inputs = keras.Input(shape=(256, 256, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "x = base_mobilenet_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(21, activation='softmax')(x)\n",
        "\n",
        "mobilenet_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# 5. Compile\n",
        "mobilenet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 6. Train\n",
        "epochs = 50\n",
        "history = mobilenet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# 7. Fine-tuning (optional but recommended)\n",
        "base_mobilenet_model.trainable = True\n",
        "# Recompile with lower learning rate\n",
        "mobilenet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "fine_tune_epochs = 50\n",
        "history_fine = mobilenet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=fine_tune_epochs\n",
        ")\n",
        "\n",
        "# 8. Evaluate\n",
        "test_loss, test_acc = mobilenet_model.evaluate(test_ds)\n",
        "print('\\nTest accuracy after fine-tuning:', test_acc)\n"
      ],
      "metadata": {
        "id": "0JSO-AFrpDkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Get predictions from the model\n",
        "for images, labels in test_ds:\n",
        "    preds = mobilenet_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Create or append to a DataFrame\n",
        "results_df.loc[len(results_df)] = [\"MobileNetV2\", accuracy, precision, recall, f1]"
      ],
      "metadata": {
        "id": "gjVYbdgNQjpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "5KqDzOhMRbUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = os.listdir('uc-merced-land-use-dataset/UCMerced_LandUse/Images')\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "cmd.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Matriz de Confusão\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a6qv4hVh4vI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de Transfer Learning para implementação de Rede Neural EfficientNetB0"
      ],
      "metadata": {
        "id": "03DkTyUYzriT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2. Data Augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "# 3. Load EfficientNetB0\n",
        "base_efficientnetb0_model = keras.applications.EfficientNetB0(\n",
        "    input_shape=(256, 256, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_efficientnetb0_model.trainable = False  # Freeze at first\n",
        "\n",
        "# 4. Build the model\n",
        "inputs = keras.Input(shape=(256, 256, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.efficientnet.preprocess_input(x)\n",
        "x = base_efficientnetb0_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "outputs = layers.Dense(21, activation='softmax')(x)\n",
        "\n",
        "efficientnetb0_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# 5. Compile\n",
        "efficientnetb0_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 6. Train\n",
        "epochs = 50\n",
        "history = efficientnetb0_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# 7. Fine-tune (optional, improves results)\n",
        "base_efficientnetb0_model.trainable = True\n",
        "efficientnetb0_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "fine_tune_epochs = 50\n",
        "history_fine = efficientnetb0_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=fine_tune_epochs\n",
        ")\n",
        "\n",
        "# 8. Evaluate\n",
        "test_loss, test_acc = efficientnetb0_model.evaluate(test_ds)\n",
        "print(\"\\nTest accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "wMdVwBwVpsxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Get predictions from the model\n",
        "for images, labels in test_ds:\n",
        "    preds = efficientnetb0_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Create or append to a DataFrame\n",
        "results_df.loc[len(results_df)] = [\"EfficientNetB0\", accuracy, precision, recall, f1]"
      ],
      "metadata": {
        "id": "LmkzAF1mRluy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "JKvR8KSYRsf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in test_ds:\n",
        "    preds = efficientnetb0_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = os.listdir('uc-merced-land-use-dataset/UCMerced_LandUse/Images')\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "cmd.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D2lb5I4kurLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de Transfer Learning para implementação de Rede Neural DenseNet121"
      ],
      "metadata": {
        "id": "YamxETU7zvo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "base_densenet121_model = keras.applications.DenseNet121(\n",
        "    input_shape=(256, 256, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_densenet121_model.trainable = False\n",
        "\n",
        "inputs = keras.Input(shape=(256, 256, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.densenet.preprocess_input(x)\n",
        "x = base_densenet121_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "outputs = layers.Dense(21, activation='softmax')(x)\n",
        "\n",
        "densenet121_model = keras.Model(inputs, outputs)\n",
        "\n",
        "densenet121_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "epochs = 50\n",
        "history = densenet121_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "base_densenet121_model.trainable = True\n",
        "densenet121_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "fine_tune_epochs = 50\n",
        "history_fine = densenet121_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=fine_tune_epochs\n",
        ")\n",
        "\n",
        "test_loss, test_acc = densenet121_model.evaluate(test_ds)\n",
        "print(\"\\nTest accuracy after fine-tuning:\", test_acc)\n"
      ],
      "metadata": {
        "id": "CKVj4OEe5mtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Get predictions from the model\n",
        "for images, labels in test_ds:\n",
        "    preds = efficientnetb0_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Create or append to a DataFrame\n",
        "results_df.loc[len(results_df)] = [\"DenseNet121\", accuracy, precision, recall, f1]\n",
        "results_df"
      ],
      "metadata": {
        "id": "lV3bwLOyTP3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for images, labels in test_ds:\n",
        "    preds = densenet121_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "# 2. Create Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = os.listdir('uc-merced-land-use-dataset/UCMerced_LandUse/Images')\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "cmd.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - DenseNet 121\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Classe Predita\")\n",
        "plt.ylabel(\"Classe Verdadeira\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1gh7uMg88EGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BML4oahGz4kF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLi46krnQzdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de Transformers visuais para obtenção de embedding do dataset"
      ],
      "metadata": {
        "id": "UzeltyZpz_IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "dataset_path = 'uc-merced-land-use-dataset/UCMerced_LandUse/Images'\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 5. Load dataset\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Optional: split into train/val/test (simple random split)\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class_names = full_dataset.classes\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# 6. Load ViT\n",
        "vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "vit_model.head = torch.nn.Identity()  # Remove classification head\n",
        "vit_model = vit_model.to(device)\n",
        "vit_model.eval()\n",
        "\n",
        "# 7. Extract embeddings\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        embeddings = vit_model(images)  # Directly pass images\n",
        "        all_embeddings.append(embeddings.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "# Stack results\n",
        "all_embeddings = torch.cat(all_embeddings).numpy()\n",
        "all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "print(f\"Embeddings shape: {all_embeddings.shape}\")\n",
        "\n",
        "# 8. Dimensionality Reduction (PCA + t-SNE)\n",
        "pca = PCA(n_components=50)\n",
        "pca_result = pca.fit_transform(all_embeddings)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=3000, random_state=42)\n",
        "tsne_result = tsne.fit_transform(pca_result)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eDPaYyvr0Ezk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_embeddings[0].shape"
      ],
      "metadata": {
        "id": "IoRmN_rI_h2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define custom distinct colors again\n",
        "custom_colors = [\n",
        "    '#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00',\n",
        "    '#ffff33', '#a65628', '#f781bf', '#999999', '#66c2a5',\n",
        "    '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854', '#ffd92f',\n",
        "    '#e5c494', '#b3b3b3', '#1b9e77', '#d95f02', '#7570b3',\n",
        "    '#e7298a'\n",
        "]\n",
        "\n",
        "# 2. Plot t-SNE embeddings\n",
        "plt.figure(figsize=(16, 9))  # Slide format 16:9\n",
        "\n",
        "colors_for_points = [custom_colors[label] for label in all_labels]\n",
        "\n",
        "scatter = plt.scatter(\n",
        "    tsne_result[:, 0],\n",
        "    tsne_result[:, 1],\n",
        "    c=colors_for_points,\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# 3. Add annotations: annotate the mean position of each class\n",
        "for i, class_name in enumerate(translated_classes_names):\n",
        "    idxs = np.where(all_labels == i)\n",
        "    if len(idxs[0]) > 0:\n",
        "        mean_x = np.mean(tsne_result[idxs, 0])\n",
        "        mean_y = np.mean(tsne_result[idxs, 1])\n",
        "        plt.text(\n",
        "            mean_x,\n",
        "            mean_y,\n",
        "            class_name,\n",
        "            fontsize=8,\n",
        "            weight='bold',\n",
        "            ha='center',\n",
        "            va='center',\n",
        "            bbox=dict(facecolor='white', alpha=0.6, edgecolor='black', boxstyle='round,pad=0.2')\n",
        "        )\n",
        "\n",
        "# 4. Custom Legend — two columns\n",
        "handles = [\n",
        "    mpatches.Patch(color=custom_colors[i], label=translated_classes_names[i])\n",
        "    for i in range(len(translated_classes_names))\n",
        "]\n",
        "\n",
        "plt.legend(\n",
        "    handles=handles,\n",
        "    bbox_to_anchor=(1.05, 1),\n",
        "    loc='upper left',\n",
        "    borderaxespad=0.,\n",
        "    title='Classes',\n",
        "    fontsize='small',\n",
        "    ncol=2\n",
        ")\n",
        "\n",
        "plt.title('Visualização dos Embeddings do UC Merced', fontsize=16)\n",
        "plt.xlabel('Dimensão 1')\n",
        "plt.ylabel('Dimensão 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# 5. Save\n",
        "plt.savefig('uc_merced_tsne_distinct_colors_annotated.png', dpi=300)\n",
        "plt.savefig('uc_merced_tsne_distinct_colors_annotated.pdf')\n",
        "\n",
        "# 6. Show\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Er9SJLNI4S2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de embedding para treinamento de modelos de aprendizado de máquina"
      ],
      "metadata": {
        "id": "rq8WYQ9eDou6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Dados de entrada\n",
        "X = all_embeddings  # Embeddings extraídos do Vision Transformer\n",
        "y = all_labels      # Rótulos correspondentes\n",
        "\n",
        "# 2. Divisão treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
        "\n",
        "# 3. Definição dos modelos\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Support Vector Machine (SVM)\": SVC(kernel='rbf', probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"K-Nearest Neighbors (KNN)\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Gradient Boosting\": lgb.LGBMClassifier()\n",
        "}\n",
        "\n",
        "# 5. Treinamento e avaliação dos modelos\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    results_df.loc[len(results_df)] = [name, acc, precision, recall, f1]\n",
        "    print(f\"Accuracy on Test Set: {acc:.4f}\")\n",
        "\n",
        "# 6. Exibição do DataFrame\n",
        "print(\"\\nResumo das Métricas:\")\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "-nzyKx1N7lkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação e Visualização dos resultados"
      ],
      "metadata": {
        "id": "PdD5bLFH96ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a visualization of the results_df dataframe\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming results_df is already defined as in your provided code\n",
        "\n",
        "results_df.plot(x=\"Model\", y=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"], kind=\"bar\", figsize=(10, 6))\n",
        "plt.title(\"Performance dos Modelos\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Métricas\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ly7E1OpZmdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JZ3Mol1sbf07"
      }
    }
  ]
}